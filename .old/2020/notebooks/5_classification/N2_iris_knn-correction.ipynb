{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be run on mybinder: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgricad-gitlab.univ-grenoble-alpes.fr%2Fchatelaf%2Fconference-ia/master?urlpath=lab/tree/notebooks/5_classification/N2_iris_knn.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Iris Data Set](https://en.wikipedia.org/wiki/Iris_flower_data_set) Example\n",
    "\n",
    "The data set consists of $n=150$ samples from each of three species of Iris ([Iris setosa](https://en.wikipedia.org/wiki/Iris_setosa), [Iris virginica](https://en.wikipedia.org/wiki/Iris_virginica) and [Iris versicolor](https://en.wikipedia.org/wiki/Iris_versicolor),  $50$ samples from each of these three species). Four <a href=\"Feature_(machine_learning)\">features</a>, i.e. $p=4$, were measured from each sample: the length and the width of the [sepals](https://en.wikipedia.org/wiki/Sepal) and [petals](https://en.wikipedia.org/wiki/Petal), in centimetres. Based on the combination of these four features, a classification task is to distinguish the species from each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Several conventional data set, such as the well known *Iris* data set. are included in the Sickit-learn toolbox. The first axis (lines) is the samples and the second axis is the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 150 samples with 4 features. We can plot the data for some pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669d7da8f1974da0ad28b5083893f3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'petal width (cm)')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "f,((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2,figsize=(16, 12))\n",
    "# Axe 1 and 2\n",
    "ax1.scatter(data[:,0], data[:,1],c=iris.target,)\n",
    "ax1.set_xlabel(iris.feature_names[0])\n",
    "ax1.set_ylabel(iris.feature_names[1])\n",
    "\n",
    "# Axe 1 and 3\n",
    "ax2.scatter(data[:,0], data[:,2],c=iris.target,)\n",
    "ax2.set_xlabel(iris.feature_names[0])\n",
    "ax2.set_ylabel(iris.feature_names[2])\n",
    "\n",
    "# Axe 2 and 4\n",
    "ax3.scatter(data[:,1], data[:,3],c=iris.target,)\n",
    "ax3.set_xlabel(iris.feature_names[1])\n",
    "ax3.set_ylabel(iris.feature_names[3])\n",
    "\n",
    "# Axe 3 and 4\n",
    "ax4.scatter(data[:,2], data[:,3],c=iris.target,)\n",
    "ax4.set_xlabel(iris.feature_names[2])\n",
    "ax4.set_ylabel(iris.feature_names[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "The problem is to learn a function linking the observation (here *data*) and the external data (here *target*). With scikit-learn, we need to select one alorithm then apply the *fit* method and the *predict* method. Using a K-Nearest Neighbors classifiers, it reduces to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1 1 2 1 1 2 2 2 2 2]\n",
      "[0 0 0 0 0 1 1 1 1 1 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier() # Creation of the classifier\n",
    "knn.fit(iris.data, iris.target) # Model fitting\n",
    "y_predict = knn.predict(iris.data) # Prediction\n",
    "\n",
    "print(y_predict[::10])\n",
    "print(iris.target[::10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, easy. But in practice we never do that: we should fit and predict with different samples. We will see later that it leads to a biased estimation of the true error rate of our algorithm. We need to split the data set into a training set and a validation set. Hopefully, scikit-learn provides a set of tools to manipulate the data. Furthermore, since the *fit* and the *predict* function are always the same whatever the algorithm, benchmarking using scikit-learn is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors for LDA: \t 0.04\n",
      "Errors for QDA: \t 0.04\n",
      "Errors for Logistic Regression: \t 0.07\n",
      "Errors for Support Vector Machines: \t 0.03\n",
      "Errors for K Nearest Neighbors: \t 0.04\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, svm\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy as sp\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Init classifier\n",
    "classifiers = [LinearDiscriminantAnalysis(solver=\"lsqr\"), QuadraticDiscriminantAnalysis(),\n",
    "               linear_model.LogisticRegression(solver='lbfgs',multi_class='multinomial'),svm.SVC(kernel='linear'), KNeighborsClassifier()]\n",
    "names = [\"LDA\", \"QDA\", \"Logistic Regression\", \"Support Vector Machines\", \"K Nearest Neighbors\"]\n",
    "\n",
    "# Split data -> 1/2 for learning & 1/2 for validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.5, random_state=0)\n",
    "\n",
    "for clf,name in zip(classifiers,names):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test) # predict the label of X_test from X_train and y_train\n",
    "    mcr_error = 1-accuracy_score(y_test, y_pred) # Compute the overall misclassification rate\n",
    "    print('Errors for {1}: \\t {0:.2f}'.format(mcr_error,name)) # \\t means \"tabular\" space, and {0:.2f} \n",
    "                                                           # means we print only two first decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model complexity\n",
    "Each algorithm has a set of hyperparameters that needs to be optimized for the considered data set. This issue will be addressed later. Here, we just illustrate the fact that they could have a great influence on the final decision.\n",
    "\n",
    "For the KNN, the number of neighors considered for the decision is the main hyperparemeter to set-up for small to medium size problem. Default value for scikit-learn is set to 5 (do knn? for help). In the following we are going to plot the overall accuracy function of the hyperparemeter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        KNeighborsClassifier\n",
       "\u001b[0;31mString form:\u001b[0m\n",
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')\n",
       "\u001b[0;31mFile:\u001b[0m        ~/miniconda3/envs/calc/lib/python3.7/site-packages/sklearn/neighbors/classification.py\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Classifier implementing the k-nearest neighbors vote.\n",
       "\n",
       "Read more in the :ref:`User Guide <classification>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "n_neighbors : int, optional (default = 5)\n",
       "    Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
       "\n",
       "weights : str or callable, optional (default = 'uniform')\n",
       "    weight function used in prediction.  Possible values:\n",
       "\n",
       "    - 'uniform' : uniform weights.  All points in each neighborhood\n",
       "      are weighted equally.\n",
       "    - 'distance' : weight points by the inverse of their distance.\n",
       "      in this case, closer neighbors of a query point will have a\n",
       "      greater influence than neighbors which are further away.\n",
       "    - [callable] : a user-defined function which accepts an\n",
       "      array of distances, and returns an array of the same shape\n",
       "      containing the weights.\n",
       "\n",
       "algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n",
       "    Algorithm used to compute the nearest neighbors:\n",
       "\n",
       "    - 'ball_tree' will use :class:`BallTree`\n",
       "    - 'kd_tree' will use :class:`KDTree`\n",
       "    - 'brute' will use a brute-force search.\n",
       "    - 'auto' will attempt to decide the most appropriate algorithm\n",
       "      based on the values passed to :meth:`fit` method.\n",
       "\n",
       "    Note: fitting on sparse input will override the setting of\n",
       "    this parameter, using brute force.\n",
       "\n",
       "leaf_size : int, optional (default = 30)\n",
       "    Leaf size passed to BallTree or KDTree.  This can affect the\n",
       "    speed of the construction and query, as well as the memory\n",
       "    required to store the tree.  The optimal value depends on the\n",
       "    nature of the problem.\n",
       "\n",
       "p : integer, optional (default = 2)\n",
       "    Power parameter for the Minkowski metric. When p = 1, this is\n",
       "    equivalent to using manhattan_distance (l1), and euclidean_distance\n",
       "    (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
       "\n",
       "metric : string or callable, default 'minkowski'\n",
       "    the distance metric to use for the tree.  The default metric is\n",
       "    minkowski, and with p=2 is equivalent to the standard Euclidean\n",
       "    metric. See the documentation of the DistanceMetric class for a\n",
       "    list of available metrics.\n",
       "\n",
       "metric_params : dict, optional (default = None)\n",
       "    Additional keyword arguments for the metric function.\n",
       "\n",
       "n_jobs : int or None, optional (default=None)\n",
       "    The number of parallel jobs to run for neighbors search.\n",
       "    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
       "    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
       "    for more details.\n",
       "    Doesn't affect :meth:`fit` method.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> X = [[0], [1], [2], [3]]\n",
       ">>> y = [0, 0, 1, 1]\n",
       ">>> from sklearn.neighbors import KNeighborsClassifier\n",
       ">>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
       ">>> neigh.fit(X, y) # doctest: +ELLIPSIS\n",
       "KNeighborsClassifier(...)\n",
       ">>> print(neigh.predict([[1.1]]))\n",
       "[0]\n",
       ">>> print(neigh.predict_proba([[0.9]]))\n",
       "[[0.66666667 0.33333333]]\n",
       "\n",
       "See also\n",
       "--------\n",
       "RadiusNeighborsClassifier\n",
       "KNeighborsRegressor\n",
       "RadiusNeighborsRegressor\n",
       "NearestNeighbors\n",
       "\n",
       "Notes\n",
       "-----\n",
       "See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
       "for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
       "\n",
       ".. warning::\n",
       "\n",
       "   Regarding the Nearest Neighbors algorithms, if it is found that two\n",
       "   neighbors, neighbor `k+1` and `k`, have identical distances\n",
       "   but different labels, the results will depend on the ordering of the\n",
       "   training data.\n",
       "\n",
       "https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = sp.arange(1,25)\n",
    "errors_train, errors_validation = [], []\n",
    "for n_ in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = knn.predict(X_train)\n",
    "    errors_train.append(1-accuracy_score(y_train, y_pred))\n",
    "    \n",
    "    y_pred = knn.predict(X_test)\n",
    "    errors_validation.append(1-accuracy_score(y_test, y_pred))\n",
    "    \n",
    "plt.plot(neighbors,errors_train)\n",
    "plt.plot(neighbors,errors_validation)\n",
    "plt.legend([\"Train\", \"Test\"])\n",
    "plt.xlabel(\"Number of neighbors\")\n",
    "plt.ylabel(\"Misclassification rate\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "- What is the (estimated) optimal value for the knn parameter?\n",
    "- What is the optimal error rate?\n",
    "- What happens when we change the test and train sets (e.g., try on several runs by changing the `random_state` seed)? \n",
    "- How confident are you on these best scores/hyperparameter values?\n",
    "\n",
    "We will see later how to apply more efficient procedures to assess the model performances or to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters value: 9\n",
      "Best test error: 0.026666666666666616\n"
     ]
    }
   ],
   "source": [
    "n_best = neighbors[sp.argmin(errors_validation)]\n",
    "print(\"Best hyperparameters value: {}\".format(n_best))\n",
    "print(\"Best test error: {}\".format(errors_validation[sp.argmin(errors_validation)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
